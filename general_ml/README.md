# Online Reading

1. [Machine Learning Mastery](https://machinelearningmastery.com/)
    1. [How to Choose Loss Functions When Training Deep Learning Neural Networks](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)
    2. [A Gentle Introduction to Cross-Entropy for Machine Learning](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
2. [What is a Siamese Neural Netowrk?](https://towardsdatascience.com/what-is-a-siamese-neural-network-b0dbeb1c6db7)
3. [Residual Neural Network - ResNet](https://iq.opengenus.org/residual-neural-networks/)
4. [Introducing Pathways: A next-generation AI architecture](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/?utm_source=pocket_mylist)
5. [PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation)
6. [ResNets â€” Residual Blocks & Deep Residual Learning](https://towardsdatascience.com/resnets-residual-blocks-deep-residual-learning-a231a0ee73d2)
7. [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)
8. [How to run your own LLM GPT](https://blog.rfox.eu/en/Programming/How_to_run_your_own_LLM_GPT.html)
9. [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
10. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/?ref=txt.cohere.com)
11. [Google "We Have No Moat, And Neither Does OpenAI](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
